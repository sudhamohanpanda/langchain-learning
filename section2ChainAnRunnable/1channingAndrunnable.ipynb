{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dd39cc3",
   "metadata": {},
   "source": [
    "## Understanding Chaining And Runnables ##\n",
    "## Load Env ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001e9429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LangSmith Configuration ===\n",
      "Endpoint: https://api.smith.langchain.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment first\n",
    "load_dotenv('./../.env')\n",
    "\n",
    "# Debug: Print to verify values are loaded\n",
    "print(\"=== LangSmith Configuration ===\")\n",
    "print(f\"Endpoint: {os.getenv('LANGSMITH_ENDPOINT')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1d0a7",
   "metadata": {},
   "source": [
    "## Create an LLM Object ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f99af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen2.5:latest\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306bd51",
   "metadata": {},
   "source": [
    "## Understating Runnable and Chaining ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17251700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Running a Large Language Model (LLM) on your local machine can offer several advantages, but it also comes with certain trade-offs. Here are some key benefits:\\n\\n1. **Control and Privacy**: Running an LLM locally means you have full control over the data used by the model. This is particularly important for sensitive or proprietary information that should not be transmitted to external servers.\\n\\n2. **Latency Reduction**: Local inference reduces latency because there's no need to send requests to a remote server, which can introduce network delays. This is especially beneficial in real-time applications where response speed is critical.\\n\\n3. **Offline Use**: You can use the LLM even when you don't have an internet connection or if network connectivity is unreliable.\\n\\n4. **Customization and Integration**: Local deployment allows for easier integration with other local software systems, custom data preprocessing pipelines, and specific business logic that may be difficult to implement in a cloud environment.\\n\\n5. **Cost Efficiency**: While some LLMs require significant computational resources, running them locally can reduce or eliminate ongoing cloud costs if you already have the necessary hardware.\\n\\n6. **Data Security**: Data security is enhanced because sensitive data does not leave your local network, reducing the risk of unauthorized access during transmission.\\n\\nHowever, it's important to note that local deployment also comes with challenges such as higher initial setup and maintenance costs, requiring powerful hardware, and handling larger datasets which might be impractical without cloud services. Additionally, updates and scaling become more complex in a local environment compared to managed cloud services.\", additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2026-02-02T18:44:21.05289Z', 'done': True, 'done_reason': 'stop', 'total_duration': 74699390933, 'load_duration': 8076891564, 'prompt_eval_count': 29, 'prompt_eval_duration': 5014566340, 'eval_count': 310, 'eval_duration': 61066031201, 'logprobs': None, 'model_name': 'qwen2.5:latest', 'model_provider': 'ollama'}, id='lc_run--019c1faa-63ef-73f1-a456-0662b13f01fa-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 29, 'output_tokens': 310, 'total_tokens': 339})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"What is advantage of running LLM in {place}\")\n",
    "                                       ])\n",
    "# Without chaining\n",
    "# prompt = Prompt_template.invoke(place=\"local machine\")\n",
    "# print(prompt)\n",
    "# content = llm.invoke(prompt)\n",
    "\n",
    "# With chaining\n",
    "chain = prompt_template | llm\n",
    "chain.invoke({\"place\":\"local machine\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb97b93",
   "metadata": {},
   "source": [
    "## String Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e20043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a Large Language Model (LLM) on your local machine has several advantages, although it also comes with some limitations. Here are some key benefits:\n",
      "\n",
      "1. **Privacy and Data Security**:\n",
      "   - **No Data Exposure**: Running the model locally means that all input data remains on your device, reducing the risk of exposing sensitive information.\n",
      "   - **Compliance with Regulations**: This can be particularly important for handling personal or confidential data in compliance with regulations like GDPR.\n",
      "\n",
      "2. **Control and Customization**:\n",
      "   - **Fine-Tuning**: You have full control over fine-tuning the model to suit your specific needs, including custom datasets and parameters.\n",
      "   - **Custom Logic**: Implementing additional features or logic that can be integrated directly into the model's workflow.\n",
      "\n",
      "3. **Speed and Performance**:\n",
      "   - **Reduced Latency**: No need for internet connections means faster response times when querying the model locally.\n",
      "   - **Efficient Use of Resources**: Direct access to local hardware resources (CPU, GPU) without network overhead.\n",
      "\n",
      "4. **Offline Capability**:\n",
      "   - **No Internet Dependency**: The model can be used even in environments with poor or no internet connectivity.\n",
      "\n",
      "5. **Cost Efficiency**:\n",
      "   - **Reduced Cloud Costs**: You avoid the ongoing costs associated with cloud services for running and hosting LLMs.\n",
      "   - **Resource Utilization**: Optimized use of local hardware resources, which might be underutilized otherwise.\n",
      "\n",
      "6. **Custom Environment Configuration**:\n",
      "   - **Tailored Software Stack**: You can configure your environment to include specific tools or libraries that are not available in cloud services.\n",
      "\n",
      "7. **Scalability and Portability**:\n",
      "   - **Flexible Deployment**: Easily deploy the model on different local machines without needing cloud infrastructure.\n",
      "   - **Portability**: The model can be moved from one machine to another more easily than moving a cloud-based service.\n",
      "\n",
      "### Limitations\n",
      "\n",
      "- **Resource Intensive**: LLMs require significant computational power, which might not always be available on standard consumer-grade hardware.\n",
      "- **Model Size**: Large models can take up substantial disk space and memory, which may limit the number of models you can run simultaneously.\n",
      "- **Maintenance and Updates**: You are responsible for maintaining and updating the model and its dependencies.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Running an LLM locally is advantageous in scenarios where privacy, control, performance, and cost efficiency are critical. However, it requires a robust local setup and careful consideration of resource requirements.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"What is advantage of running LLM in {place}\")\n",
    "                                       ])\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "output = chain.invoke({\"place\":\"local machine\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea21060",
   "metadata": {},
   "source": [
    "## Understanding and working with multiple chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de209a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Advantages of Running a Large Language Model (LLM) Locally**\n",
      "- **Considerations and Limitations for Running a Large Language Model Locally**\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"What is advantage of running LLM in {place}\")])\n",
    "\n",
    "# Chain 1\n",
    "detailOutputChain = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "headingInfoChain = ChatPromptTemplate.from_template(\"\"\"\n",
    "                                                    Analyze the Output{output} and just generate heading only.\n",
    "                                                    Response should be in bulleted format.\n",
    "                                                    \"\"\")\n",
    "#Chain 2\n",
    "chainWithHeading = detailOutputChain | headingInfoChain | llm | StrOutputParser()\n",
    "output = chainWithHeading.invoke({\"place\":\"local machine\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605c0b4",
   "metadata": {},
   "source": [
    "## Understanding and working on RunnableParallel ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739110d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
