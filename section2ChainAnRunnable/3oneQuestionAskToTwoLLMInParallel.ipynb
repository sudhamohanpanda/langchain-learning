{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95be535e",
   "metadata": {},
   "source": [
    "## Parralel Running ##\n",
    "- Will create three llm with three different models\n",
    "- will ask one same question to two different llm and compare the result with third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5823d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint: https://api.smith.langchain.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('./../.env')\n",
    "\n",
    "# Debug: Print to verify values are loaded\n",
    "print(f\"Endpoint: {os.getenv('LANGSMITH_ENDPOINT')}\")\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm1 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"qwen2.5:latest\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "llm2 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "llm3 = ChatOllama(\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    model=\"DeepSeek-R1\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c67d2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:Okay, here is a comparison question formatted for your parameters:\n",
      "\n",
      "`Please ask this question: \"What are the key principles of sustainable urban planning and how do they address climate change?\"`\n",
      "\n",
      "Answer from qwen2.5: Certainly! Hereâ€™s how you can frame the question based on your provided format:\n",
      "\n",
      "**Question:** What are the key principles of sustainable urban planning and how do they address climate change?\n",
      "\n",
      "Answer from mistral:  What are the essential principles of sustainable urban planning and in what ways do they contribute to addressing climate change?\n",
      "\n",
      "Sustainable urban planning seeks to create cities that are livable, resilient, and environmentally friendly. Here are some key principles:\n",
      "\n",
      "1. Compact city form: This principle advocates for densely populated cities with a mix of land uses (residential, commercial, and recreational) within walking distance, reducing the need for cars and thus lowering greenhouse gas emissions.\n",
      "\n",
      "2. Mixed-use development: This involves combining residential, commercial, and cultural activities in the same areas, creating vibrant neighborhoods that encourage walking and public transportation use.\n",
      "\n",
      "3. Public transport-oriented design: This principle emphasizes the importance of investing in efficient and affordable public transportation systems to reduce car usage and greenhouse gas emissions.\n",
      "\n",
      "4. Green infrastructure: This includes parks, green roofs, and urban forests, which help absorb carbon dioxide, reduce stormwater runoff, and provide habitats for wildlife.\n",
      "\n",
      "5. Energy efficiency: Sustainable urban planning promotes energy-efficient buildings and renewable energy sources to lower cities' overall carbon footprint.\n",
      "\n",
      "6. Climate resilience: This principle focuses on creating cities that can adapt to and recover from extreme weather events, such as floods and heatwaves, which are becoming more common due to climate change.\n",
      "\n",
      "7. Equity and social sustainability: Sustainable urban planning aims to create inclusive cities where all residents have access to quality housing, education, healthcare, and public spaces. This principle recognizes that addressing climate change requires taking into account the needs of vulnerable populations who are often disproportionately affected by its impacts.\n",
      "\n",
      "By incorporating these principles, sustainable urban planning can help reduce greenhouse gas emissions, improve air and water quality, promote healthier lifestyles, and create more resilient communities in the face of a changing climate.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "prompt_template1 = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"I want to ask a question to two different LLMs. Then want to compare the result. Can you genrate that question. Pleaese return only question so that I can in parameter.\")])\n",
    "\n",
    "chain1 = prompt_template1 | llm3 | StrOutputParser()\n",
    "question = chain1.invoke({}) \n",
    "print(\"Q:\"+question)\n",
    "\n",
    "prompt_template2 = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"{question}\")])\n",
    "\n",
    "chain2 = prompt_template2 | llm1 | StrOutputParser()\n",
    "chain3 = prompt_template2 | llm2 | StrOutputParser()\n",
    "\n",
    "# Create parallel runnable with meaningful names\n",
    "parallelRunnable = RunnableParallel(\n",
    "    qwen_answer=chain2, \n",
    "    mistral_answer=chain3\n",
    ")\n",
    "\n",
    "# Run both chains in parallel\n",
    "response = parallelRunnable.invoke({\"question\": question})\n",
    "\n",
    "# Extract individual responses\n",
    "answer1 = response[\"qwen_answer\"]\n",
    "answer2 = response[\"mistral_answer\"]\n",
    "\n",
    "print(f\"\\nAnswer from qwen2.5: {answer1}\")\n",
    "print(f\"\\nAnswer from mistral: {answer2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99b2356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final comparison result:\n",
      "Okay, here is the comparison:\n",
      "\n",
      "`Please ask this question: \"What are the key principles of sustainable urban planning and how do they address climate change?\"`\n",
      "\n",
      "The **better answer** is **Answer 1**, primarily based on its direct adherence to the user's requested format.\n",
      "\n",
      "Here's a breakdown justifying why Answer 1 is better (according to my role as an LLM expert analyzing the provided response):\n",
      "\n",
      "| Feature          | Answer 1                                    | Answer 2                                                      |\n",
      "| :--------------- | :------------------------------------------ | :------------------------------------------------------------ |\n",
      "| **Directness**    | Directly answers the user's question format. | Does not directly answer the \"how\" part of the question in the requested format; instead, provides a pre-answered example that doesn't incorporate the specific principles it mentions into climate change explanations. |\n",
      "| **Accuracy to Request** | Matches the structure: identifies principles and states they address climate change (though the explanation focuses on the *principles themselves*). | Provides an overly detailed answer but does not frame it as a direct response following the exact \"Question\" format requested by the user (\"Please ask this question\"). |\n",
      "| **Focus**         | Concentrates on listing principles relevant to sustainability and clarifying their climate connection. | While informative, deviates from being a pure \"answer\" following the specific 'ask' structure provided in Answer 1. |\n",
      "\n",
      "Therefore, Answer 1 is more suitable as it directly fulfills the instruction given by the user (to answer *that specific question*).\n"
     ]
    }
   ],
   "source": [
    "# Now compare the two answers using the third LLM\n",
    "prompt_template3 = ChatPromptTemplate([\n",
    "    (\"system\",\"You are an LLM expert\"),\n",
    "    (\"user\",\"Compare the two answers for {question} and give me the better one. \\n Answer1: {Answer1} \\n Answer2: {Answer2}\")])    \n",
    "\n",
    "chain4 = prompt_template3 | llm3 | StrOutputParser()\n",
    "final_answer = chain4.invoke({\n",
    "    \"question\": question, \n",
    "    \"Answer1\": answer1, \n",
    "    \"Answer2\": answer2\n",
    "})\n",
    "print(f\"\\nFinal comparison result:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590db200",
   "metadata": {},
   "source": [
    "## Alternative Ways to Use RunnableParallel\n",
    "\n",
    "Here are different patterns for collecting responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b61161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using dictionary unpacking\n",
    "parallel_chains = RunnableParallel(\n",
    "    model_a=chain2,\n",
    "    model_b=chain3\n",
    ")\n",
    "results = parallel_chains.invoke({\"question\": question})\n",
    "model_a_response, model_b_response = results[\"model_a\"], results[\"model_b\"]\n",
    "\n",
    "print(\"Method 1 - Dictionary access:\")\n",
    "print(f\"Model A: {model_a_response}\")\n",
    "print(f\"Model B: {model_b_response}\")\n",
    "\n",
    "# Method 2: Using get() method for safer access\n",
    "model_a_safe = results.get(\"model_a\", \"No response\")\n",
    "model_b_safe = results.get(\"model_b\", \"No response\")\n",
    "\n",
    "print(f\"\\nMethod 2 - Safe access:\")\n",
    "print(f\"Model A: {model_a_safe}\")\n",
    "print(f\"Model B: {model_b_safe}\")\n",
    "\n",
    "# Method 3: Iterate through all results\n",
    "print(f\"\\nMethod 3 - Iterate through all:\")\n",
    "for model_name, response in results.items():\n",
    "    print(f\"{model_name}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d74ae",
   "metadata": {},
   "source": [
    "## Advanced RunnableParallel Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: More complex parallel processing\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_metadata(response):\n",
    "    return {\n",
    "        \"content\": response,\n",
    "        \"length\": len(response),\n",
    "        \"word_count\": len(response.split())\n",
    "    }\n",
    "\n",
    "enhanced_parallel = RunnableParallel(\n",
    "    qwen_enhanced=chain2 | RunnableLambda(add_metadata),\n",
    "    mistral_enhanced=chain3 | RunnableLambda(add_metadata)\n",
    ")\n",
    "\n",
    "enhanced_results = enhanced_parallel.invoke({\"question\": question})\n",
    "\n",
    "print(\"Enhanced results with metadata:\")\n",
    "for model, data in enhanced_results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Content: {data['content'][:100]}...\")\n",
    "    print(f\"  Length: {data['length']} chars\")\n",
    "    print(f\"  Words: {data['word_count']} words\")\n",
    "\n",
    "# Pattern 2: Combining parallel results directly\n",
    "combined_parallel = RunnableParallel(\n",
    "    responses=RunnableParallel(\n",
    "        qwen=chain2,\n",
    "        mistral=chain3\n",
    "    )\n",
    ")\n",
    "\n",
    "nested_results = combined_parallel.invoke({\"question\": question})\n",
    "print(f\"\\nNested structure: {list(nested_results.keys())}\")\n",
    "print(f\"Nested responses: {list(nested_results['responses'].keys())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
